# -*- coding: utf-8 -*-
"""
Created on Mon Mar 30 16:58:35 2015

@author: Yan
"""

import numpy as np

repers = open('repers_endless.txt', 'w')                     #создание файла
repers.close()

phi0 = [0,  1.36,  -0.49, 0.36]                              #параметры авторегрессионных моделей
phi1 = [0,  1.02,  -0.40, 0.63]                              #последнее число - параметр B, определяющий
phi2 = [0,  0.82,  -0.49, 0.73]                              #                      степень влияния шума
phi3 = [0,  0,     -0.49, 0.87]
phi4 = [0, -0.82,  -0.49, 0.73]
phi = np.array([phi0,phi1,phi2,phi3,phi4])
n = 2                                                        #порядок авторегрессии
m = 5                                                        #количество различных классов{моделей}

q1 = [0.99,   0.0025, 0.0025, 0.0025, 0.0025]
q2 = [0.0025, 0.99,   0.0025, 0.0025, 0.0025]
q3 = [0.0025, 0.0025, 0.99,   0.0025, 0.0025]
q4 = [0.0025, 0.0025, 0.0025, 0.99,   0.0025]
q5 = [0.0025, 0.0025, 0.0025, 0.0025, 0.99]
Q = np.array([q1,q2,q3,q4,q5])                              #матрица вер-ти переходов

curr_t = -1                                                 #текущий номер
x = []                                                      #список для хранения отсчетов
start_x = []                                                #список для стартовых значений

##########################################################################################
############################Блок генератора модельных данных##############################
##########################################################################################

while(curr_t != n-1 ):                                      #начальное наполнение случайными данными
    #x.append(np.random.randn())                             #т.к. AR(n) основывается на n пред.значениях
    start_x.append(np.random.randn())                       #то же самое, но в массиве, не влияющем на x
    curr_t += 1

#Bозможно, что начальное наполнение случайными данными внесет искажения в начале анализа данных, поэтому я
#создаю дополнительный массив, на котором основывается авторегрессия до момента заполнения x n значениями
#N_start = curr_t + 1                                        #номер, с которого начнутся модельные данные
curr_t = -1                                                 #сбрасываем счетчик после наполнения
h_start = 0                                                 #начальный выбор модели

def generate_number(h_curr):                                #функция генератора
    repers = open('repers_endless.txt', 'a')                #файл точек, где происходит смена модели
    #для смены модели пропорционально вероятности, я решил брать рандомное число, принадлежащее [0,1],
    #разбить этот отрезок на сегменты, равные, собственно, вероятностям и смотреть, в какой сегмент
    #случайное число попало                                          
    factor = np.random.random()
    p_curr = 0
    global curr_t                                           #работаем с глобальным счетчиком отсчетов
    curr_t += 1
    for h_next in range(0,m):
        p_curr += Q[h_curr][h_next]
        if(p_curr >= factor):
            if(h_curr != h_next):
                print('Model change: N', curr_t, h_curr, '-->', h_next)
                repers.write(str(curr_t) + ' ' + str(h_curr) + '\n')        #конец предыдущего сегмента
                repers.write(str(curr_t) + ' ' + str(h_next) + '\n')        #начало следующего
                h_curr = h_next
            break
    #цикл формирования отсчета AR(n)-процесса
    x_curr = phi[h_curr][0]
    for i in range(1,n+1):
        if(curr_t - i >= 0):                                                #проверка на отрицательный
            x_curr += phi[h_curr][i]*x[curr_t - i]                          #индекс
        else:
            x_curr += phi[h_curr][i]*start_x[curr_t - i]                    #используем стартовый массив
            print('start_x has been used, curr_t = ' + str(curr_t))         #debug
    x_curr += phi[h_curr][3]*np.random.randn()              #шум    
    #заменил random.random() на random.randn() - он имеет нормальное распределение, почему-то стало
                                                                                             #лучше
    x.append(x_curr)
    print(x_curr,'h_curr =',h_curr)
    repers.close()
    return h_curr                                           #возвращаем текущую модель
    
##########################################################################################
############################Блок анализатора данных#######################################
##########################################################################################

def beta(t, h_t1, h_t):                                     #ф-ция beta
    summ = 0                                                #сначала надо посчитать сумму, зав. от x
    for i in range(1,n+1):
        if(t - i >= 0):                                     #проверка на отрицательный индекс                   
            summ += x[t-i]*phi[h_t][i]
        else:
            summ += start_x[t-i]*phi[h_t][i]                #используем массив start_x
    res = (1/(2*phi[h_t][3]))*(x[t] - phi[h_t][0] - summ)**2 - np.log(Q[h_t1][h_t]) #по формуле из статьи
    return res

def d_t(d_t1):                                              #рекуррентный расчет d_t
    res = []
    k_t = []
    for i in range(0,m):                                    #перебор по моделям
        curr_j = []                                         #посчитать для всех j и выбрать min
        for j in range(0,m):                                #перебор по выбору предыдущего
            #beta_curr = beta(curr_t,j,i)
            curr_j.append(d_t1[j] + beta(curr_t,j,i))       #расчет вектора всех значений, среди которых
        res.append(np.min(curr_j))                          #ищется минимум
        k_t.append(np.argmin(curr_j))                       #вектор k_t
    return res, k_t                                         #возвращаем рассчитанные вектора

def g_t(g_t1, k_t):                                         #рекуррентный расчет g_t
    res = []
    for i in range(0,m):
        res.append(g_t1[k_t[i]])
    return res

##########################################################################################
########################Главный цикл выполнения программы#################################
##########################################################################################

#u = N_start - 1                                             #правая граница принятия решений
u = -1
#t_special = N_start                                         #особый момент времени    
t_special = 0
K = []                                                      #матрица K
#начальный вектор d_t0
d_t0 = [-np.log(0.99),-np.log(0.0025),-np.log(0.0025),-np.log(0.0025),-np.log(0.0025)]
g_t0 = []
for i in range(0,m):                                        #формирование вектора g_t0
    g_t0.append(i)
for i in range(0,5):                                     #3000 - тест, в конце перейти на while(True)
    h_start = generate_number(h_start)                      #передача текущей модели в генератор
    d_t0, k_t_curr = d_t(d_t0)                              #получаем k_t(i) и d_t(i)
    K.append(k_t_curr)                                      #добавляем рассчитанный столбец в матрицу K
    